{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 60-Minute Blitz\n",
    "\n",
    "This notebook is to jot down some thoughts as I'm going through the 60-Minute Blitz tutorial [here](http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2724.7168     0.0000 -2724.7168\n",
      "    0.0000     0.0000     0.0000\n",
      "    0.0000     0.0000     0.0000\n",
      "    0.0000    -0.0000     0.0000\n",
      "    0.0000     0.0000    -0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: tensors can be uninitialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5033  0.7657  0.7389\n",
      " 0.8549  0.7376  0.9325\n",
      " 0.7186  0.5029  0.1061\n",
      " 0.9886  0.8077  0.0406\n",
      " 0.4225  0.5819  0.6325\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kinds of adding, without changing `x` or `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "print(x.add(y))\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kinds of adding, where one of the variables is mutated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xclone = x.clone() # So we don't mess up x\n",
    "xclone.add_(y)\n",
    "print(xclone) # Should be == x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xclone = x.clone()\n",
    "torch.add(xclone, y, out=xclone)\n",
    "print(xclone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing, and stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5033  0.7657  0.7389\n",
      " 0.8549  0.7376  0.9325\n",
      " 0.7186  0.5029  0.1061\n",
      " 0.9886  0.8077  0.0406\n",
      " 0.4225  0.5819  0.6325\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.5033\n",
      " 0.8549\n",
      " 0.7186\n",
      " 0.9886\n",
      " 0.4225\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      " 0.8549\n",
      " 0.7376\n",
      " 0.9325\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 0.5029\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:, 0])\n",
    "print(x[1, :])\n",
    "print(x[2:3, 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: slices are, as expected, inclusive in first and exclusive in second parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors in PyTorch are stored in an underlying numpy array. This numpy array can be changed obtained and modified--and the tensor variable will reflect those changes (and vice-versa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: scalars addition with tensors is handled conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build a tensor variable on top of an existing numpy array, and the tensor will use the existing numpy array as its underlying storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)\n",
    "# a and b should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting variable on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      " 0.5080  1.2187  1.3362\n",
      " 1.1371  1.1508  1.6654\n",
      " 1.3650  1.2020  0.7618\n",
      " 1.5371  1.7909  0.7624\n",
      " 0.6864  1.5570  0.7882\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts:\n",
    "\n",
    "The underscore after an operation (e.g., `x.add_(y)`) means in-place. Does `torch.add(x, y, out=x)` also do it in-place (i.e., it optimizes this case automatically)? Either way, for in-place, it's more clear to use `_` notation, I think. \n",
    "\n",
    "The `_` notation doesn't make sense to use with `torch.add`, since in-place would be ambiguous. Lo and behold, as expected, it's note defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'torch' has no attribute 'add_'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    torch.add_(x, y)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We observed a few things:\n",
    " * creating a tensor;\n",
    " * operations on a tensor, which can be\n",
    "    - with mutations (in-place, when the syntax makes sense, or using `out` keyword parameter), or\n",
    "    - without mutations (`x.add(y)`, `torch.add(x, y)`, `x + y`);\n",
    " * tensors are represented with an underlying numpy array:\n",
    "   - operations to one are reflected in the other, and\n",
    "   - we can go back and forth between them (`x.numpy()`, `torch.from_numpy(a)`);\n",
    " * and, finally, we can put tensors on the GPU (`x = x.cuda()`) and do operations on them (`x + y`) with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: closer to the good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = autograd.Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddConstantBackward object at 0x7f15b8172b88>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "y.grad = None\n",
    "x.grad = None\n",
    "grad_wrt_downstream_scalar_output = torch.ones(2, 2)\n",
    "y.backward(grad_wrt_downstream_scalar_output)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to summarize the concepts of `autograd` so far:\n",
    "* all gradients are computed w.r.t. some eventual downstream scalar output, so:\n",
    "  - when computing `y.backward()` where `y` is a scalar, no arguments are needed (equivalent to `y.backward(torch.Tensor([1.]))`), but\n",
    "  - when computing `y.backward()` where `y` is a tensor, the gradient of the eventual downstream scalar output `z` with respect to `y` must be given as an argument to the `backward()` function;\n",
    "* and, there two main classes in autograd, which are `Variable` and `Function`:\n",
    "  - Variable is a symbolic variable with state, and\n",
    "  - Function is...something (not entirely clear from the tutorial) which is used (at least) for computing downstream gradients (Function is probably some object which stores references to the upstream `Variable`s--which were used in calculatations to create the new `Variable`--as well as how to compute the gradient of the current `Variable` with respect to the immediately-upstream `Variable`s; however, I cannot say for sure yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad = None\n",
    "y.grad = None\n",
    "z.grad = None\n",
    "out.grad = None\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify by hand, where $x$ is `x`, $y$ is `y`, $z$ is `z`, and $o$ is `out`.\n",
    "$$o = \\frac{1}{4} \\sum_{i=1}^4 3(x_i + 2)^2$$\n",
    "$$\\frac{\\partial o}{\\partial x_i} = 1.5(x_i + 2)$$\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\Bigr|_{x = 1} = 1.5(3) = 4.5$$\n",
    "We see that our program is correct! Very cool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do \"many crazy things\", too (direct quote from tutorial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 512  512  512\n",
      " 512  512  512\n",
      " 512  512  512\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.randn(3, 3), requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "y.sum().backward()\n",
    "print(x.grad) # Should be some power of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, two few more things:\n",
    "* first, if no `Variable`s in a graph are initialized with `requires_grad=True`, then no gradients will be computed on a call to `backward()` (and it will raise an exception);\n",
    "* second, gradients are accumulated at the leaf nodes in the acyclic computation graph (i.e., the ones which were created by the user), and persist between calls to `backward()`.\n",
    "\n",
    "### Practice: learning to detect symmetry\n",
    "\n",
    "I'll take a page from one of the original neural net backprop papers by Rumelhart et al. (1986, Nature) and learn to detect symmetry in a binary vector $v \\in \\{0,1\\}^{2n}$ using a two-layer neural network. \n",
    "\n",
    "First, let's define our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  1.]\n",
      " [ 0.  0.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  1.  1.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  1.]\n",
      " [ 0.  0.  1.  1.  1.  0.]\n",
      " [ 0.  0.  1.  1.  1.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan) # So I can print whole array.\n",
    "\n",
    "def fill_binary_numbers(arr, start_index=0, prev_digits=[]):\n",
    "    \"\"\"Fills 2-d numpy array with binary numbers, counting upwards from 0, 1, 10, 11, etc.\"\"\"\n",
    "    for power in range(0, arr.shape[1]):\n",
    "        for i in range(0, arr.shape[0]):\n",
    "            arr[i, arr.shape[1] - power - 1] = int(i/(2**power)) % 2\n",
    "\n",
    "dataset = np.zeros((64, 6), dtype=\"float32\")\n",
    "fill_binary_numbers(dataset)\n",
    "print(dataset[0:17, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our dataset, we need to get our labels. (0 indicates symmetry, 1 indicates asymmetry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.89999998  0.89999998  0.89999998  0.89999998  0.89999998  0.89999998\n",
      "  0.89999998  0.89999998  0.89999998  0.89999998  0.89999998  0.89999998\n",
      " -0.89999998  0.89999998  0.89999998  0.89999998  0.89999998]\n"
     ]
    }
   ],
   "source": [
    "labels = np.logical_not(\n",
    "    np.all(\n",
    "        np.equal(dataset[:, 0:3], dataset[:, 5:2:-1]), axis=1)).astype(\"float32\")\n",
    "labels = (labels - 0.5)*1.8\n",
    "print(labels[0:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes from debugging:\n",
    "* `==` can't be used for element-wise array comparison, so use `np.equal(a, b)` instead;\n",
    "* numpy arrays use `.shape` to store the shape tuple, and `.size` to store the number of elements in total.\n",
    "\n",
    "Now that we've got our labels, let's make our neural net function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_inputs = autograd.Variable(torch.from_numpy(dataset))\n",
    "labels = autograd.Variable(torch.from_numpy(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2609  0.6436\n",
      " 0.2619  0.3725\n",
      " 0.4759  0.5814\n",
      " 0.4771  0.3135\n",
      " 0.2135  0.8076\n",
      " 0.2144  0.5799\n",
      " 0.4112  0.7636\n",
      " 0.4124  0.5150\n",
      " 0.2841  0.7472\n",
      " 0.2851  0.4928\n",
      " 0.5051  0.6946\n",
      " 0.5064  0.4278\n",
      " 0.2338  0.8730\n",
      " 0.2347  0.6932\n",
      " 0.4397  0.8410\n",
      " 0.4410  0.6349\n",
      " 0.1599  0.5721\n",
      "[torch.FloatTensor of size 17x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.4968\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_l1 = autograd.Variable(torch.randn(6, 2), requires_grad=True)\n",
    "biases_l1 = autograd.Variable(torch.randn(1, 2), requires_grad=True)\n",
    "weights_l2 = autograd.Variable(torch.randn(2, 1), requires_grad=True)\n",
    "biases_l2 = autograd.Variable(torch.randn(1, 1), requires_grad=True)\n",
    "\n",
    "hidden_layer = torch.sigmoid(torch.matmul(binary_inputs, weights_l1) + biases_l1)\n",
    "print(hidden_layer.data[0:17])\n",
    "\n",
    "output_layer = torch.sigmoid(torch.matmul(hidden_layer, weights_l2) + biases_l2).squeeze()\n",
    "\n",
    "error = sum((labels - output_layer).pow(2))/64.0\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us proceed to the actual learning portion. We'll use $\\epsilon = 0.1$ as our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3772\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-213.4112 -118.8703\n",
      "-331.1634  -68.7048\n",
      "-256.3842  -54.3993\n",
      "-272.2270  -85.8507\n",
      "-263.9906  -37.8422\n",
      "-233.3955   13.7417\n",
      "[torch.FloatTensor of size 6x2]\n",
      "\n",
      "Variable containing:\n",
      "-143.9969  174.5171\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-455.7071\n",
      " -20.2008\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "Variable containing:\n",
      "-0.9131\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "\n",
    "for i in range(10000):\n",
    "    hidden_layer = torch.sigmoid(torch.matmul(binary_inputs, weights_l1) - biases_l1)\n",
    "    output_layer = torch.sigmoid(torch.matmul(hidden_layer, weights_l2) - biases_l2).squeeze()\n",
    "    error = sum((labels - output_layer).pow(2))/64.0\n",
    "    error.backward()\n",
    "    \n",
    "    weights_l1.data -= eps * weights_l1.grad.data\n",
    "    biases_l1.data -= eps * biases_l1.grad.data\n",
    "    weights_l2.data -= eps * weights_l2.grad.data\n",
    "    biases_l2.data -= eps * biases_l2.grad.data\n",
    "\n",
    "hidden_layer = torch.sigmoid(torch.matmul(binary_inputs, weights_l1))\n",
    "output_layer = torch.sigmoid(torch.matmul(hidden_layer, weights_l2))\n",
    "output_layer.squeeze_()\n",
    "error = sum((labels - output_layer).pow(2))/64.0\n",
    "\n",
    "print(error)\n",
    "print(weights_l1)\n",
    "print(biases_l1)\n",
    "print(weights_l2)\n",
    "print(biases_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there's something wrong here. I'll try to fix this be doing a simpler optimization problem later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
