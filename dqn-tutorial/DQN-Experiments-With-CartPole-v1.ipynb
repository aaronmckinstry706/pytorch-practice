{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Experiments With CartPole-v1\n",
    "\n",
    "In my last notebook, I tried to implement a version of Google's original DQN learning algorithm, with some modifications. In this notebook, I'll be doing the same thing, but there are two differences:\n",
    "* I want to use the not-downsampled images, and\n",
    "* I want to use a reward of -1 for the final state, and 0 for the regular states.\n",
    "First, let's define the Transition and ReplayMemory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import typing\n",
    "import torch\n",
    "\n",
    "class Transition(typing.NamedTuple):\n",
    "    prev_state: typing.Union[torch.cuda.FloatTensor]\n",
    "    prev_action: int\n",
    "    prev_reward: float\n",
    "    cur_state: typing.Union[torch.cuda.FloatTensor]\n",
    "    end_transition: bool\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    capacity: int\n",
    "    memory: typing.List[Transition]\n",
    "    position: int\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, transition: Transition):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size: int) -> typing.Iterable[Transition]:\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a quick experiment to figure out our network size. Looking at the source code for CartPole-v1, we find (after calculations I won't write here) that the pole occupies pixels 160:300 row-wise and all the pixels column-wise. So we'll extract this from the image in our `resize` function, and use it as input to the neural network. \n",
    "\n",
    "With that out of the way, we just have to calculate the size of the linear layer just before the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 14, 71])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = autograd.Variable(torch.zeros(1, 3, 140, 600))\n",
    "x = nn.Conv2d(3, 32, 8, stride=4)(x)\n",
    "x = nn.Conv2d(32, 64, 4, stride=2)(x)\n",
    "x = nn.Conv2d(64, 64, 3, stride=1)(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too big! Wayyyyy to big. We need the linear layer to be a lot smaller, not 64x16x71 times tha batch size. Let's add a layer to reduce it down to the intended 84x84 of the original paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 65, 74])\n",
      "torch.Size([1, 64, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = autograd.Variable(torch.zeros(1, 3, 140, 600))\n",
    "x = nn.Conv2d(3, 16, 12, stride=(2, 8))(x)\n",
    "print(x.size())\n",
    "x = nn.Conv2d(16, 32, 8, stride=4)(x)\n",
    "x = nn.Conv2d(32, 64, 4, stride=2)(x)\n",
    "x = nn.Conv2d(64, 64, 3, stride=1)(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it's not quite 84x84, but I think it'll work just fine. The final linear hidden layer is smaller, but we'll try it anyway--so its size is 64x4x5, or 1280 hidden units. \n",
    "\n",
    "With that, we can define our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "\n",
    "class SimpleAtariNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAtariNet, self).__init__()\n",
    "        # Assume input is size 140x600\n",
    "        self.conv0 = nn.Conv2d(3, 16, 12, stride=(2, 8))\n",
    "        self.conv1 = nn.Conv2d(16, 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        self.lin1 = nn.Linear(1280, 512)\n",
    "        self.lin2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.conv0(x))\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        x = functional.relu(self.conv3(x))\n",
    "        x = functional.relu(self.lin1(x.view(-1, 1280)))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "def test_net():\n",
    "    assert SimpleAtariNet()(autograd.Variable(torch.ones(1, 3, 140, 600))).size() == (1, 2)\n",
    "\n",
    "test_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define our training context, but with a few alterations from our previous version. \n",
    "* Changed learning rate to 0.00025;\n",
    "* Changed annealing length on $\\epsilon$ to 1000000. \n",
    "* Changed the network to accept high-resolution inputs and then downsample appropriately (see above). \n",
    "* Changed the reward to be 0, except for -1 at termination state. \n",
    "* Changed steps before updating target network to 10000. \n",
    "* Added frame-skipping with frequency of 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 22:28:01,971] Making new env: CartPole-v1\n",
      "[2017-11-23 22:28:13,143] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "class CartPoleDQNContextSimple(object):\n",
    "    def __init__(self, memory_size: int, cuda: bool):\n",
    "        self.cuda = cuda\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.replay_memory = ReplayMemory(memory_size)\n",
    "        self.net = SimpleAtariNet()\n",
    "        self.targetnet = SimpleAtariNet()\n",
    "        self.targetnet.load_state_dict(self.net.state_dict())\n",
    "        if self.cuda:\n",
    "            self.net.cuda()\n",
    "            self.targetnet.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.net.parameters(), lr=0.00025, weight_decay = 0.001)\n",
    "        self.batch_size = 32\n",
    "        self.c = 10000\n",
    "        self.resize = transforms.Compose(\n",
    "            [transforms.ToPILImage(),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Lambda(lambda x: x[:, 160:300, :])])\n",
    "        self.discount_factor = 0.99\n",
    "        self.num_steps_with_current_target = 0\n",
    "        self.init_epsilon = 0.1\n",
    "        self.final_epsilon = 0.9\n",
    "        self.anneal_length = 1000000\n",
    "        self.num_steps = 0\n",
    "        self.frame_skip_frequency = 4\n",
    "    \n",
    "    def do_action(self, action: int):\n",
    "        for i in range(self.frame_skip_frequency):\n",
    "            _, reward, done, _ = self.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        return reward, done\n",
    "        \n",
    "    def run_episode(self):\n",
    "        self.env.reset()\n",
    "        prev_screen = self.get_screen()\n",
    "        prev_state = self.get_screen()\n",
    "        prev_action = self.choose_action(prev_state)\n",
    "        prev_reward, done = self.do_action(prev_action)\n",
    "        while not done:\n",
    "            cur_screen = self.get_screen()\n",
    "            cur_state = cur_screen - prev_screen\n",
    "            \n",
    "            cur_epsilon = self.init_epsilon + (self.final_epsilon - self.init_epsilon)*self.num_steps/self.anneal_length\n",
    "            if random.random() > 1 - cur_epsilon:\n",
    "                cur_action = self.env.action_space.sample()\n",
    "            else:\n",
    "                cur_action = self.choose_action(cur_state)\n",
    "            \n",
    "            cur_reward, done = self.do_action(cur_action)\n",
    "            \n",
    "            if done:\n",
    "                cur_reward = -1 # Modified because of problem with constant reward. \n",
    "            else:\n",
    "                cur_reward = 0\n",
    "            \n",
    "            # Add transition.\n",
    "            self.replay_memory.push(Transition(prev_state=prev_state,\n",
    "                                               prev_action=prev_action,\n",
    "                                               prev_reward=prev_reward,\n",
    "                                               cur_state=cur_state,\n",
    "                                               end_transition=False))\n",
    "            \n",
    "            # If we're done, we also need to add the next Transition with an end state.\n",
    "            if done:\n",
    "                self.replay_memory.push(Transition(prev_state=cur_state,\n",
    "                                                   prev_action=cur_action,\n",
    "                                                   prev_reward=cur_reward,\n",
    "                                                   cur_state=self.get_screen(),\n",
    "                                                   end_transition=True))\n",
    "            # Now that we've added transitions, we can sample self.batch_size number of replay transitions\n",
    "            # and perform a step in the optimizer.\n",
    "            cur_batch_size = min(len(self.replay_memory), self.batch_size)\n",
    "            transitions = self.replay_memory.sample(cur_batch_size)\n",
    "            states, actions, rewards, next_states, end_transitions = zip(*transitions)\n",
    "            \n",
    "            states = autograd.Variable(torch.cat([state.unsqueeze(0) for state in states])) # Add batch dim to each sample.\n",
    "            rewards = autograd.Variable(torch.cat([torch.Tensor([reward]) for reward in rewards]))\n",
    "            end_transitions = autograd.Variable(torch.Tensor(end_transitions).float())\n",
    "            next_states = autograd.Variable(torch.cat([next_state.unsqueeze(0) for next_state in next_states]))\n",
    "            \n",
    "            actions_mask = torch.zeros(cur_batch_size, 2).byte()\n",
    "            for sample_index, action in enumerate(actions):\n",
    "                actions_mask[sample_index, action] = 1\n",
    "            \n",
    "            if self.cuda:\n",
    "                states = states.cuda()\n",
    "                next_states = next_states.cuda()\n",
    "                rewards = rewards.cuda()\n",
    "                end_transitions = end_transitions.cuda()\n",
    "                actions_mask = actions_mask.cuda()\n",
    "            \n",
    "            target_qvalues = self.targetnet(next_states)[actions_mask]\n",
    "            if self.cuda:\n",
    "                target_qvalues = target_qvalues.cuda()\n",
    "            current_qvalues, _ = self.net(states).max(1)\n",
    "            \n",
    "            error = torch.sum(\n",
    "                (rewards + self.discount_factor*target_qvalues*end_transitions - current_qvalues)**2)/cur_batch_size\n",
    "            self.optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            self.optimizer.step()\n",
    "            self.num_steps_with_current_target += 1\n",
    "            if self.num_steps_with_current_target == self.c:\n",
    "                self.targetnet.load_state_dict(self.net.state_dict())\n",
    "                self.num_steps_with_current_target = 0\n",
    "            \n",
    "            # After all that is done, we reset our context variables for the next iteration.\n",
    "            prev_screen = cur_screen\n",
    "            prev_state = cur_state\n",
    "            prev_action = cur_action\n",
    "            prev_reward = cur_reward\n",
    "            \n",
    "            self.num_steps += 1\n",
    "    \n",
    "    def get_screen(self):\n",
    "        screen = self.resize(self.env.render(mode='rgb_array')).float()/255.0\n",
    "        return screen\n",
    "    \n",
    "    def choose_action(self, observation: torch.Tensor) -> typing.Tuple[autograd.Variable, int]:\n",
    "        \"\"\"Returns the action chosen by the network.\"\"\"\n",
    "        network_input = autograd.Variable(observation.unsqueeze(0))\n",
    "        if self.cuda:\n",
    "            network_input = network_input.cuda()\n",
    "        value, index = self.net(network_input).max(1)\n",
    "        return index.data[0]\n",
    "\n",
    "def test_context():\n",
    "    CartPoleDQNContextSimple(50000, True).run_episode()\n",
    "    CartPoleDQNContextSimple(50000, False).run_episode()\n",
    "\n",
    "test_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out a few SGD steps and time it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 22:28:13,482] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74134.5175899376\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def time_sgd(n: int):\n",
    "    context = CartPoleDQNContextSimple(10000, True)\n",
    "    t1 = datetime.datetime.now()\n",
    "    for i in range(n):\n",
    "        context.run_episode()\n",
    "    t2 = datetime.datetime.now()\n",
    "    sgd_steps_per_hour = context.num_steps/(t2 - t1).total_seconds()*3600\n",
    "    print(str(sgd_steps_per_hour))\n",
    "\n",
    "time_sgd(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this for 50 million iterations requires that we run this for nearly a month (31 days). However, to run this for 750k iterations only requires 8 hours, or just one night. *That* is doable. So, we'll run it for 8 hours, and see what happens in the morning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 22:28:31,248] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "def train_and_save(iterations: int):\n",
    "    context = CartPoleDQNContextSimple(10000, True)\n",
    "    while context.num_steps < iterations:\n",
    "        context.run_episode()\n",
    "    torch.save(context.net, './cartpole-net')\n",
    "\n",
    "train_and_save(8*70000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to stop this in the middle of running, because everything slowed to a stop. I'll have to run this in a separate script. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_env]",
   "language": "python",
   "name": "conda-env-ml_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
