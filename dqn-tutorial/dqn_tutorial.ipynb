{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Tutorial\n",
    "\n",
    "This notebook is for me to continue my practice with PyTorch--while learning about how to train a Deep Q-Network! Exciting stuff. I just finished reviewing the chapters on reinforcement learning (chapters 17 & 21) in the textbook \"Artificial Intelligence A Modern Approach\", 3rd ed. (Russel & Norvig). While it isn't the most up-to-date reference on the subject, it does give the foundations of reinforcement learning--in particular, it covers the temporal difference update equation, which is used as an objective function in a DQN. \n",
    "\n",
    "So: here goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import typing\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So *this* is how easy it is to set up a gym environment. That's super cool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transition(typing.NamedTuple):\n",
    "    state: torch.Tensor\n",
    "    action: torch.Tensor\n",
    "    next_state: torch.Tensor\n",
    "    reward: torch.Tensor\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    capacity: int\n",
    "    memory: typing.List[Transition]\n",
    "    position: int\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, transition: Transition):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size: int) -> typing.Iterable[Transition]:\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = functional.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I forget to write it down, I'm trying to use type hints throughout the code. It makes everything clearer in my head (because, contrary to popular belief in the Python community, you *can't* stop worrying about types in Python). Here are some things I learned so far:\n",
    "* `typing` has its own `NamedTuple` class from which you create a subclass, but this allows you to put type hints on your variables;\n",
    "* user-defined classes can be used as-is, without creating a special type variable, in type hints;\n",
    "* there is no type hint for having no return type (you just don't use a type hint);\n",
    "* return types for functions are written like `def function(...) -> returnType: blablacodeblabla`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, back to the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyplot.ion()\n",
    "\n",
    "resize = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.Scale(40, interpolation=PIL.Image.CUBIC),\n",
    "     transforms.ToTensor()])\n",
    "\n",
    "screen_width = 600\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    screen = screen[:, :, slice_range]\n",
    "    screen = numpy.ascontiguousarray(screen, dtype=numpy.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "env.reset()\n",
    "pyplot.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "pyplot.title('Example extracted screen')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state: torch.Tensor) -> typing.Union[LongTensor, autograd.Variable]:\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "    -1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(\n",
    "            autograd.Variable(state, volatile=True).type(FloatTensor)\n",
    "        ).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    pyplot.figure(2)\n",
    "    pyplot.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    pyplot.title('Training...')\n",
    "    pyplot.xlabel('Episode')\n",
    "    pyplot.ylabel('Duration')\n",
    "    pyplot.plot(durations_t.numpy())\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        pyplot.plot(means.numpy())\n",
    "    \n",
    "    pyplot.pause(0.001)\n",
    "    import IPython\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(pyplot.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_sync = 0\n",
    "\n",
    "def optimize_model():\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)))\n",
    "    non_final_next_states = autograd.Variable(\n",
    "        torch.cat([s for s in batch.next_state if s is not None]),\n",
    "        volatile=True)\n",
    "    state_batch = autograd.Variable(torch.cat(batch.state))\n",
    "    action_batch = autograd.Variable(torch.cat(batch.action))\n",
    "    reward_batch = autograd.Variable(torch.cat(batch.reward))\n",
    "    \n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "    next_state_values = autograd.Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "    next_state_values.volatile = False\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    loss = functional.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in range(1000):\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action[0, 0])\n",
    "        reward = Tensor([reward])\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        memory.push(Transition(state, action, next_state, reward))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "pyplot.ioff()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND THAT'S IT! Going through this tutorial was mostly a way to prove to myself that this stuff is easier than I thought (as in, I have enough technical background to understand the details of what's going on). I think the next thing to do is try to really do well in this simple game. Also, try a few modifications:\n",
    "* maybe I'll just give it the image instead of the difference between two subsequent images;\n",
    "* try incorporating a recurrent network...somehow (probably have to read up again on that before I tried it);\n",
    "* try different network architectures;\n",
    "* try slight training variations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
